## Supervised Learning in R: Regression
#Chap-1-what-is-regression
> # unemployment is loaded in the workspace
> summary(unemployment)
 male_unemployment female_unemployment
 Min.   :2.900     Min.   :4.000      
 1st Qu.:4.900     1st Qu.:4.400      
 Median :6.000     Median :5.200      
 Mean   :5.954     Mean   :5.569      
 3rd Qu.:6.700     3rd Qu.:6.100      
 Max.   :9.800     Max.   :7.900
> 
> # Define a formula to express female_unemployment as a function of male_unemployment
> fmla <- female_unemployment~male_unemployment
> 
> # Print it
> fmla
female_unemployment ~ male_unemployment
> 
> # Use the formula to fit a model: unemployment_model
> unemployment_model <- lm(fmla,unemployment)
> 
> # Print it
> unemployment_model

Call:
lm(formula = fmla, data = unemployment)

Coefficients:
      (Intercept)  male_unemployment  
           1.4341             0.6945
  > # unemployment is loaded in the workspace
> summary(unemployment)
 male_unemployment female_unemployment
 Min.   :2.900     Min.   :4.000      
 1st Qu.:4.900     1st Qu.:4.400      
 Median :6.000     Median :5.200      
 Mean   :5.954     Mean   :5.569      
 3rd Qu.:6.700     3rd Qu.:6.100      
 Max.   :9.800     Max.   :7.900
> 
> # Define a formula to express female_unemployment as a function of male_unemployment
> fmla <- female_unemployment~male_unemployment
> 
> # Print it
> fmla
female_unemployment ~ male_unemployment
> 
> # Use the formula to fit a model: unemployment_model
> unemployment_model <- lm(fmla,unemployment)
> 
> # Print it
> unemployment_model

Call:
lm(formula = fmla, data = unemployment)

Coefficients:
      (Intercept)  male_unemployment  
           1.4341             0.6945
  # unemployment is in your workspace
> summary(unemployment)
 male_unemployment female_unemployment
 Min.   :2.900     Min.   :4.000      
 1st Qu.:4.900     1st Qu.:4.400      
 Median :6.000     Median :5.200      
 Mean   :5.954     Mean   :5.569      
 3rd Qu.:6.700     3rd Qu.:6.100      
 Max.   :9.800     Max.   :7.900
> 
> # newrates is in your workspace
> newrates
  male_unemployment
1                 5
> 
> # Predict female unemployment in the unemployment data set
> unemployment$prediction <-  predict(unemployment_model)
> 
> # load the ggplot2 package
> library(ggplot2)
> 
> # Make a plot to compare predictions to actual (prediction on x axis).
> ggplot(unemployment, aes(x = prediction, y = female_unemployment)) + 
   geom_point() +
    geom_abline(color = "blue")
> 
> # Predict female unemployment rate when male unemployment is 5%
> pred <- predict(unemployment_model,newrates)
> # Print it
> pred
       1 
4.906757
# bloodpressure is in the workspace
> summary(bloodpressure)
 blood_pressure       age            weight   
 Min.   :128.0   Min.   :46.00   Min.   :167  
 1st Qu.:140.0   1st Qu.:56.50   1st Qu.:186  
 Median :153.0   Median :64.00   Median :194  
 Mean   :150.1   Mean   :62.45   Mean   :195  
 3rd Qu.:160.5   3rd Qu.:69.50   3rd Qu.:209  
 Max.   :168.0   Max.   :74.00   Max.   :220
> 
> # Create the formula and print it
> fmla <- blood_pressure~age+weight
> fmla
blood_pressure ~ age + weight
> 
> # Fit the model: bloodpressure_model
> bloodpressure_model <- lm(fmla,bloodpressure)
> 
> # Print bloodpressure_model and call summary()
> bloodpressure_model

Call:
lm(formula = fmla, data = bloodpressure)

Coefficients:
(Intercept)          age       weight  
    30.9941       0.8614       0.3349
> summary(bloodpressure_model)

Call:
lm(formula = fmla, data = bloodpressure)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.4640 -1.1949 -0.4078  1.8511  2.6981 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)  30.9941    11.9438   2.595  0.03186 * 
age           0.8614     0.2482   3.470  0.00844 **
weight        0.3349     0.1307   2.563  0.03351 * 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.318 on 8 degrees of freedom
Multiple R-squared:  0.9768,	Adjusted R-squared:  0.9711 
F-statistic: 168.8 on 2 and 8 DF,  p-value: 2.874e-07
 # bloodpressure is in your workspace
> summary(bloodpressure)
 blood_pressure       age            weight   
 Min.   :128.0   Min.   :46.00   Min.   :167  
 1st Qu.:140.0   1st Qu.:56.50   1st Qu.:186  
 Median :153.0   Median :64.00   Median :194  
 Mean   :150.1   Mean   :62.45   Mean   :195  
 3rd Qu.:160.5   3rd Qu.:69.50   3rd Qu.:209  
 Max.   :168.0   Max.   :74.00   Max.   :220
> 
> # bloodpressure_model is in your workspace
> bloodpressure_model

Call:
lm(formula = fmla, data = bloodpressure)

Coefficients:
(Intercept)          age       weight  
    30.9941       0.8614       0.3349
> 
> # predict blood pressure using bloodpressure_model :prediction
> bloodpressure$prediction <- predict(bloodpressure_model)
> 
> # plot the results
> ggplot(bloodpressure, aes(x = prediction, y = blood_pressure)) + 
       geom_point() + 
      geom_abline(color = "blue")
#chap-2-Training and Evaluating Regression Models
 # unemployment, unemployment_model are in the workspace
> summary(unemployment)
 male_unemployment female_unemployment
 Min.   :2.900     Min.   :4.000      
 1st Qu.:4.900     1st Qu.:4.400      
 Median :6.000     Median :5.200      
 Mean   :5.954     Mean   :5.569      
 3rd Qu.:6.700     3rd Qu.:6.100      
 Max.   :9.800     Max.   :7.900
> summary(unemployment_model)

Call:
lm(formula = fmla, data = unemployment)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.77621 -0.34050 -0.09004  0.27911  1.31254 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        1.43411    0.60340   2.377   0.0367 *  
male_unemployment  0.69453    0.09767   7.111 1.97e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5803 on 11 degrees of freedom
Multiple R-squared:  0.8213,	Adjusted R-squared:  0.8051 
F-statistic: 50.56 on 1 and 11 DF,  p-value: 1.966e-05
> 
> # Make predictions from the model
> unemployment$predictions <- predict(unemployment_model)
> 
> # Fill in the blanks to plot predictions (on x-axis) versus the female_unemployment rates
> ggplot(unemployment, aes(x = predictions, y = female_unemployment)) + 
    geom_point() + 
    geom_abline()
  # unemployment, unemployment_model are in the workspace
> summary(unemployment)
 male_unemployment female_unemployment
 Min.   :2.900     Min.   :4.000      
 1st Qu.:4.900     1st Qu.:4.400      
 Median :6.000     Median :5.200      
 Mean   :5.954     Mean   :5.569      
 3rd Qu.:6.700     3rd Qu.:6.100      
 Max.   :9.800     Max.   :7.900
> summary(unemployment_model)

Call:
lm(formula = fmla, data = unemployment)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.77621 -0.34050 -0.09004  0.27911  1.31254 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        1.43411    0.60340   2.377   0.0367 *  
male_unemployment  0.69453    0.09767   7.111 1.97e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5803 on 11 degrees of freedom
Multiple R-squared:  0.8213,	Adjusted R-squared:  0.8051 
F-statistic: 50.56 on 1 and 11 DF,  p-value: 1.966e-05
> 
> # Make predictions from the model
> unemployment$predictions <- predict(unemployment_model)
> 
> # Fill in the blanks to plot predictions (on x-axis) versus the female_unemployment rates
> ggplot(unemployment, aes(x = predictions, y = female_unemployment)) + 
    geom_point() + geom_abline()
# unemployment is in the workspace
> summary(unemployment)
 male_unemployment female_unemployment  predictions      residuals       
 Min.   :2.900     Min.   :4.000       Min.   :3.448   Min.   :-1.31254  
 1st Qu.:4.900     1st Qu.:4.400       1st Qu.:4.837   1st Qu.:-0.27911  
 Median :6.000     Median :5.200       Median :5.601   Median : 0.09004  
 Mean   :5.954     Mean   :5.569       Mean   :5.569   Mean   : 0.00000  
 3rd Qu.:6.700     3rd Qu.:6.100       3rd Qu.:6.087   3rd Qu.: 0.34050  
 Max.   :9.800     Max.   :7.900       Max.   :8.240   Max.   : 0.77621
> 
> # For convenience put the residuals in the variable res
> res <- unemployment$residuals
> 
> # Calculate RMSE, assign it to the variable rmse and print it
> (rmse <- sqrt(mean(res^2)))
[1] 0.5337612
> 
> # Calculate the standard deviation of female_unemployment and print it
> (sd_unemployment <- sd(unemployment$female_unemployment))
[1] 1.314271    
# unemployment is in your workspace
> summary(unemployment)
 male_unemployment female_unemployment  predictions      residuals       
 Min.   :2.900     Min.   :4.000       Min.   :3.448   Min.   :-1.31254  
 1st Qu.:4.900     1st Qu.:4.400       1st Qu.:4.837   1st Qu.:-0.27911  
 Median :6.000     Median :5.200       Median :5.601   Median : 0.09004  
 Mean   :5.954     Mean   :5.569       Mean   :5.569   Mean   : 0.00000  
 3rd Qu.:6.700     3rd Qu.:6.100       3rd Qu.:6.087   3rd Qu.: 0.34050  
 Max.   :9.800     Max.   :7.900       Max.   :8.240   Max.   : 0.77621
> 
> # unemployment_model is in the workspace
> summary(unemployment_model)

Call:
lm(formula = fmla, data = unemployment)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.77621 -0.34050 -0.09004  0.27911  1.31254 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        1.43411    0.60340   2.377   0.0367 *  
male_unemployment  0.69453    0.09767   7.111 1.97e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5803 on 11 degrees of freedom
Multiple R-squared:  0.8213,	Adjusted R-squared:  0.8051 
F-statistic: 50.56 on 1 and 11 DF,  p-value: 1.966e-05
> 
> # Get the correlation between the prediction and true outcome: rho and print it
> (rho <- cor(unemployment$predictions,unemployment$female_unemployment))
[1] 0.9062647
> 
> # Square rho: rho2 and print it
> (rho2 <- rho^2)
[1] 0.8213157
> 
> # Get R-squared from glance and print it
> (rsq_glance <- glance(unemployment_model)$r.squared)
[1] 0.8213157
# mpg is in the workspace
> summary(mpg)
 manufacturer          model               displ            year     
 Length:234         Length:234         Min.   :1.600   Min.   :1999  
 Class :character   Class :character   1st Qu.:2.400   1st Qu.:1999  
 Mode  :character   Mode  :character   Median :3.300   Median :2004  
                                       Mean   :3.472   Mean   :2004  
                                       3rd Qu.:4.600   3rd Qu.:2008  
                                       Max.   :7.000   Max.   :2008  
      cyl           trans               drv                 cty       
 Min.   :4.000   Length:234         Length:234         Min.   : 9.00  
 1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  
 Median :6.000   Mode  :character   Mode  :character   Median :17.00  
 Mean   :5.889                                         Mean   :16.86  
 3rd Qu.:8.000                                         3rd Qu.:19.00  
 Max.   :8.000                                         Max.   :35.00  
      hwy             fl               class          
 Min.   :12.00   Length:234         Length:234        
 1st Qu.:18.00   Class :character   Class :character  
 Median :24.00   Mode  :character   Mode  :character  
 Mean   :23.44                                        
 3rd Qu.:27.00                                        
 Max.   :44.00
> dim(mpg)
[1] 234  11
> 
> # Use nrow to get the number of rows in mpg (N) and print it
> (N <- nrow(mpg))
[1] 234
> 
> # Calculate how many rows 75% of N should be and print it
> # Hint: use round() to get an integer
> (target <- round(N * 0.75))
[1] 176
> 
> # Create the vector of N uniform random variables: gp
> gp <- runif(N)
> 
> # Use gp to create the training set: mpg_train (75% of data) and mpg_test (25% of data)
> mpg_train <- mpg[gp < 0.75, ]
> mpg_test <- mpg[gp >= 0.75, ]
> 
> # Use nrow() to examine mpg_train and mpg_test
> nrow(mpg_train)
[1] 180
> nrow(mpg_test)
[# mpg_train is in the workspace
> summary(mpg_train)
 manufacturer          model               displ            year     
 Length:180         Length:180         Min.   :1.600   Min.   :1999  
 Class :character   Class :character   1st Qu.:2.500   1st Qu.:1999  
 Mode  :character   Mode  :character   Median :3.400   Median :2008  
                                       Mean   :3.558   Mean   :2004  
                                       3rd Qu.:4.600   3rd Qu.:2008  
                                       Max.   :7.000   Max.   :2008  
      cyl           trans               drv                 cty       
 Min.   :4.000   Length:180         Length:180         Min.   : 9.00  
 1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  
 Median :6.000   Mode  :character   Mode  :character   Median :16.00  
 Mean   :6.022                                         Mean   :16.58  
 3rd Qu.:8.000                                         3rd Qu.:19.00  
 Max.   :8.000                                         Max.   :33.00  
      hwy             fl               class          
 Min.   :12.00   Length:180         Length:180        
 1st Qu.:18.00   Class :character   Class :character  
 Median :24.00   Mode  :character   Mode  :character  
 Mean   :23.11                                        
 3rd Qu.:27.00                                        
 Max.   :44.00
> 
> # Create a formula to express cty as a function of hwy: fmla and print it.
> (fmla <- cty~hwy)
cty ~ hwy
> 
> # Now use lm() to build a model mpg_model from mpg_train that predicts cty from hwy
> mpg_model <- lm(fmla,mpg_train)
> 
> # Use summary() to examine the model
> summary(mpg_model)

Call:
lm(formula = fmla, data = mpg_train)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.8400 -0.8305 -0.1551  0.5865  4.8140 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.13375    0.38309   2.959   0.0035 ** 
hwy          0.66825    0.01608  41.564   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.251 on 178 degrees of freedom
Multiple R-squared:  0.9066,	Adjusted R-squared:  0.9061 
F-statistic:  1728 on 1 and 178 DF,  p-value: < 2.2e-161] 

# Examine the objects in the workspace
> ls.str()
mpg_model : List of 12
 $ coefficients : Named num [1:2] 1.134 0.668
 $ residuals    : Named num [1:180] -2.513 0.487 -1.85 -2.508 -0.508 ...
 $ effects      : Named num [1:180] -222.414 -51.994 -1.735 -2.354 -0.354 ...
 $ rank         : int 2
 $ fitted.values: Named num [1:180] 20.5 20.5 21.8 18.5 18.5 ...
 $ assign       : int [1:2] 0 1
 $ qr           :List of 5
 $ df.residual  : int 178
 $ xlevels      : Named list()
 $ call         : language lm(formula = fmla, data = mpg_train)
 $ terms        :Classes 'terms', 'formula'  language cty ~ hwy
 $ model        :'data.frame':	180 obs. of  2 variables:
mpg_test : Classes 'tbl_df', 'tbl' and 'data.frame':	54 obs. of  11 variables:
 $ manufacturer: chr  "audi" "audi" "audi" "audi" ...
 $ model       : chr  "a4" "a4" "a4 quattro" "a4 quattro" ...
 $ displ       : num  2 3.1 1.8 2 4.2 5.3 6.5 2.4 3.7 5.2 ...
 $ year        : int  2008 2008 1999 2008 2008 2008 1999 1999 2008 1999 ...
 $ cyl         : int  4 6 4 4 8 8 8 4 6 8 ...
 $ trans       : chr  "auto(av)" "auto(av)" "auto(l5)" "auto(s6)" ...
 $ drv         : chr  "f" "f" "4" "4" ...
 $ cty         : int  21 18 16 19 16 11 14 19 15 11 ...
 $ hwy         : int  30 27 25 27 23 14 17 27 19 17 ...
 $ fl          : chr  "p" "p" "p" "p" ...
 $ class       : chr  "compact" "compact" "compact" "compact" ...
mpg_train : Classes 'tbl_df', 'tbl' and 'data.frame':	180 obs. of  11 variables:
 $ manufacturer: chr  "audi" "audi" "audi" "audi" ...
 $ model       : chr  "a4" "a4" "a4" "a4" ...
 $ displ       : num  1.8 1.8 2 2.8 2.8 1.8 2 2.8 2.8 3.1 ...
 $ year        : int  1999 1999 2008 1999 1999 1999 2008 1999 1999 2008 ...
 $ cyl         : int  4 4 4 6 6 4 4 6 6 6 ...
 $ trans       : chr  "auto(l5)" "manual(m5)" "manual(m6)" "auto(l5)" ...
 $ drv         : chr  "f" "f" "f" "f" ...
 $ cty         : int  18 21 20 16 18 18 20 15 17 17 ...
 $ hwy         : int  29 29 31 26 26 26 28 25 25 25 ...
 $ fl          : chr  "p" "p" "p" "p" ...
 $ class       : chr  "compact" "compact" "compact" "compact" ...
r_squared : function (predcol, ycol)  
rmse : function (predcol, ycol)
> 
> # predict cty from hwy for the training set
> mpg_train$pred <- predict(mpg_model,mpg_train)
> 
> # predict cty from hwy for the test set
> mpg_test$pred <- predict(mpg_model,mpg_test)
> 
> # Evaluate the rmse on both training and test data and print them
> (rmse_train <- rmse(mpg_train$pred,mpg_train$cty))
[1] 1.243958
> (rmse_test <- rmse(mpg_test$pred,mpg_test$cty))
[1] 1.277228
> 
> 
> # Evaluate the r-squared on both training and test data.and print them
> (rsq_train <- r_squared(mpg_train$pred,mpg_train$cty))
[1] 0.9065908
> (rsq_test <- r_squared(mpg_test$pred,mpg_test$cty))
[1] 0.9251412
> 
> # Plot the predictions (on the x-axis) against the outcome (cty) on the test data
> ggplot(mpg_test, aes(x = pred, y = cty)) + 
    geom_point() + 
    geom_abline()
  
  # Load the package vtreat
> library(vtreat)
> 
> # mpg is in the workspace
> summary(mpg)
 manufacturer          model               displ            year     
 Length:234         Length:234         Min.   :1.600   Min.   :1999  
 Class :character   Class :character   1st Qu.:2.400   1st Qu.:1999  
 Mode  :character   Mode  :character   Median :3.300   Median :2004  
                                       Mean   :3.472   Mean   :2004  
                                       3rd Qu.:4.600   3rd Qu.:2008  
                                       Max.   :7.000   Max.   :2008  
      cyl           trans               drv                 cty       
 Min.   :4.000   Length:234         Length:234         Min.   : 9.00  
 1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  
 Median :6.000   Mode  :character   Mode  :character   Median :17.00  
 Mean   :5.889                                         Mean   :16.86  
 3rd Qu.:8.000                                         3rd Qu.:19.00  
 Max.   :8.000                                         Max.   :35.00  
      hwy             fl               class          
 Min.   :12.00   Length:234         Length:234        
 1st Qu.:18.00   Class :character   Class :character  
 Median :24.00   Mode  :character   Mode  :character  
 Mean   :23.44                                        
 3rd Qu.:27.00                                        
 Max.   :44.00
> 
> # Get the number of rows in mpg
> nRows <- nrow(mpg)
> 
> # Implement the 3-fold cross-fold plan with vtreat
> splitPlan <- kWayCrossValidation(nRows, 3, dframe=NULL, y=NULL)
> 
> # Examine the split plan
> str(splitPlan)
List of 3
 $ :List of 2
  ..$ train: int [1:156] 1 3 4 5 6 10 11 12 13 14 ...
  ..$ app  : int [1:78] 79 202 78 212 151 131 123 134 9 37 ...
 $ :List of 2
  ..$ train: int [1:156] 1 2 3 4 7 8 9 17 18 20 ...
  ..$ app  : int [1:78] 5 55 176 68 159 32 198 165 72 197 ...
 $ :List of 2
  ..$ train: int [1:156] 2 5 6 7 8 9 10 11 12 13 ...
  ..$ app  : int [1:78] 138 126 58 130 100 149 88 66 163 75 ...
 - attr(*, "splitmethod")= chr "kwaycross"
  # mpg is in the workspace
> summary(mpg)
 manufacturer          model               displ            year     
 Length:234         Length:234         Min.   :1.600   Min.   :1999  
 Class :character   Class :character   1st Qu.:2.400   1st Qu.:1999  
 Mode  :character   Mode  :character   Median :3.300   Median :2004  
                                       Mean   :3.472   Mean   :2004  
                                       3rd Qu.:4.600   3rd Qu.:2008  
                                       Max.   :7.000   Max.   :2008  
      cyl           trans               drv                 cty       
 Min.   :4.000   Length:234         Length:234         Min.   : 9.00  
 1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  
 Median :6.000   Mode  :character   Mode  :character   Median :17.00  
 Mean   :5.889                                         Mean   :16.86  
 3rd Qu.:8.000                                         3rd Qu.:19.00  
 Max.   :8.000                                         Max.   :35.00  
      hwy             fl               class          
 Min.   :12.00   Length:234         Length:234        
 1st Qu.:18.00   Class :character   Class :character  
 Median :24.00   Mode  :character   Mode  :character  
 Mean   :23.44                                        
 3rd Qu.:27.00                                        
 Max.   :44.00
> 
> # splitPlan is in the workspace
> str(splitPlan)
List of 3
 $ :List of 2
  ..$ train: int [1:156] 3 5 8 9 10 11 13 14 17 18 ...
  ..$ app  : int [1:78] 80 4 216 60 227 166 220 119 73 168 ...
 $ :List of 2
  ..$ train: int [1:156] 1 2 3 4 5 6 7 10 11 12 ...
  ..$ app  : int [1:78] 54 180 174 162 14 79 230 150 151 17 ...
 $ :List of 2
  ..$ train: int [1:156] 1 2 4 6 7 8 9 12 14 15 ...
  ..$ app  : int [1:78] 11 58 142 219 141 13 3 94 128 76 ...
 - attr(*, "splitmethod")= chr "kwaycross"
> 
> # Run the 3-fold cross validation plan from splitPlan
> k <- 3 # Number of folds
> mpg$pred.cv <- 0
> for(i in 1:k) {
    split <- splitPlan[[i]]
    model <- lm(cty ~ hwy, data = mpg[split$train, ])
    mpg$pred.cv[split$app] <- predict(model, newdata = mpg[split$app, ])
  }
> 
> # Predict from a full model
> mpg$pred <- predict(lm(cty ~ hwy, data = mpg))
> 
> # Get the rmse of the full model's predictions
> rmse(mpg$pred, mpg$cty)
[1] 1.247045
> 
> # Get the rmse of the cross-validation predictions
> rmse(mpg$pred.cv, mpg$cty)
[1] 1.260323
chap-3-Categorical inputs
 # Call str on flowers to see the types of each column
> str(flowers)
'data.frame':	24 obs. of  3 variables:
 $ Flowers  : num  62.3 77.4 55.3 54.2 49.6 61.9 39.4 45.7 31.3 44.9 ...
 $ Time     : chr  "Late" "Late" "Late" "Late" ...
 $ Intensity: int  150 150 300 300 450 450 600 600 750 750 ...
> 
> # Use unique() to see how many possible values Time takes
> unique(flowers$Time)
[1] "Late"  "Early"
> 
> # Build a formula to express Flowers as a function of Intensity and Time: fmla. Print it
> (fmla <- as.formula("Flowers ~ Intensity + Time"))
Flowers ~ Intensity + Time
> 
> # Use fmla and model.matrix to see how the data is represented for modeling
> mmat <- model.matrix(fmla, flowers)
> 
> # Examine the first 20 lines of flowers
> head(flowers, n = 20)
   Flowers  Time Intensity
1     62.3  Late       150
2     77.4  Late       150
3     55.3  Late       300
4     54.2  Late       300
5     49.6  Late       450
6     61.9  Late       450
7     39.4  Late       600
8     45.7  Late       600
9     31.3  Late       750
10    44.9  Late       750
11    36.8  Late       900
12    41.9  Late       900
13    77.8 Early       150
14    75.6 Early       150
15    69.1 Early       300
16    78.0 Early       300
17    57.0 Early       450
18    71.1 Early       450
19    62.9 Early       600
20    52.2 Early       600
> 
> # Examine the first 20 lines of mmat
> head(mmat, n = 20)
   (Intercept) Intensity TimeLate
1            1       150        1
2            1       150        1
3            1       300        1
4            1       300        1
5            1       450        1
6            1       450        1
7            1       600        1
8            1       600        1
9            1       750        1
10           1       750        1
11           1       900        1
12           1       900        1
13           1       150        0
14           1       150        0
15           1       300        0
16           1       300        0
17           1       450        0
18           1       450        0
19           1       600        0
20           1       600        0
> # flowers in is the workspace
> str(flowers)
'data.frame':	24 obs. of  3 variables:
 $ Flowers  : num  62.3 77.4 55.3 54.2 49.6 61.9 39.4 45.7 31.3 44.9 ...
 $ Time     : chr  "Late" "Late" "Late" "Late" ...
 $ Intensity: int  150 150 300 300 450 450 600 600 750 750 ...
> 
> # fmla is in the workspace
> fmla
Flowers ~ Intensity + Time
> 
> # Fit a model to predict Flowers from Intensity and Time : flower_model
> flower_model <-lm(fmla,flowers)
> 
> # Use summary on mmat to remind yourself of its structure
> summary(mmat)
  (Intercept)   Intensity      TimeLate  
 Min.   :1    Min.   :150   Min.   :0.0  
 1st Qu.:1    1st Qu.:300   1st Qu.:0.0  
 Median :1    Median :525   Median :0.5  
 Mean   :1    Mean   :525   Mean   :0.5  
 3rd Qu.:1    3rd Qu.:750   3rd Qu.:1.0  
 Max.   :1    Max.   :900   Max.   :1.0
> 
> # Use summary to examine flower_model
> summary(flower_model)

Call:
lm(formula = fmla, data = flowers)

Residuals:
   Min     1Q Median     3Q    Max 
-9.652 -4.139 -1.558  5.632 12.165 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  83.464167   3.273772  25.495  < 2e-16 ***
Intensity    -0.040471   0.005132  -7.886 1.04e-07 ***
TimeLate    -12.158333   2.629557  -4.624 0.000146 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 6.441 on 21 degrees of freedom
Multiple R-squared:  0.7992,	Adjusted R-squared:   0.78 
F-statistic: 41.78 on 2 and 21 DF,  p-value: 4.786e-08
> 
> # Predict the number of flowers on each plant
> flowers$predictions <-predict(flower_model,flowers)
> 
> # Plot predictions vs actual flowers (predictions on x-axis)
> ggplot(flowers, aes(x = predictions, y = Flowers)) + 
    geom_point() +
    geom_abline(color = "blue")
 > # alcohol is in the workspace
> summary(alcohol)
    Subject         Metabol          Gastric          Sex    
 Min.   : 1.00   Min.   : 0.100   Min.   :0.800   Female:18  
 1st Qu.: 8.75   1st Qu.: 0.600   1st Qu.:1.200   Male  :14  
 Median :16.50   Median : 1.700   Median :1.600              
 Mean   :16.50   Mean   : 2.422   Mean   :1.863              
 3rd Qu.:24.25   3rd Qu.: 2.925   3rd Qu.:2.200              
 Max.   :32.00   Max.   :12.300   Max.   :5.200              
          Alcohol  
 Alcoholic    : 8  
 Non-alcoholic:24
> 
> # Create the formula with main effects only
> (fmla_add <- Metabol~Gastric+Sex)
Metabol ~ Gastric + Sex
> 
> # Create the formula with interactions
> (fmla_interaction <- Metabol~Gastric+ Gastric:Sex )
Metabol ~ Gastric + Gastric:Sex
> 
> # Fit the main effects only model
> model_add <-lm(fmla_add,alcohol)
> 
> # Fit the interaction model
> model_interaction <- lm(fmla_interaction,alcohol)
> 
> # Call summary on both models and compare
> summary(model_add)

Call:
lm(formula = fmla_add, data = alcohol)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.2779 -0.6328 -0.0966  0.5783  4.5703 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -1.9466     0.5198  -3.745 0.000796 ***
Gastric       1.9656     0.2674   7.352 4.24e-08 ***
SexMale       1.6174     0.5114   3.163 0.003649 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.331 on 29 degrees of freedom
Multiple R-squared:  0.7654,	Adjusted R-squared:  0.7492 
F-statistic: 47.31 on 2 and 29 DF,  p-value: 7.41e-10
> summary(model_interaction)

Call:
lm(formula = fmla_interaction, data = alcohol)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.4656 -0.5091  0.0143  0.5660  4.0668 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)      -0.7504     0.5310  -1.413 0.168236    
Gastric           1.1489     0.3450   3.331 0.002372 ** 
Gastric:SexMale   1.0422     0.2412   4.321 0.000166 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.204 on 29 degrees of freedom
Multiple R-squared:  0.8081,	Adjusted R-squared:  0.7948 
F-statistic: 61.05 on 2 and 29 DF,  p-value: 4.033e-11
> # alcohol is in the workspace
> summary(alcohol)
    Subject         Metabol          Gastric          Sex    
 Min.   : 1.00   Min.   : 0.100   Min.   :0.800   Female:18  
 1st Qu.: 8.75   1st Qu.: 0.600   1st Qu.:1.200   Male  :14  
 Median :16.50   Median : 1.700   Median :1.600              
 Mean   :16.50   Mean   : 2.422   Mean   :1.863              
 3rd Qu.:24.25   3rd Qu.: 2.925   3rd Qu.:2.200              
 Max.   :32.00   Max.   :12.300   Max.   :5.200              
          Alcohol  
 Alcoholic    : 8  
 Non-alcoholic:24
> 
> # Both the formulae are in the workspace
> fmla_add
Metabol ~ Gastric + Sex
> fmla_interaction
Metabol ~ Gastric + Gastric:Sex
> 
> # Create the splitting plan for 3-fold cross validation
> set.seed(34245)  # set the seed for reproducibility
> splitPlan <- kWayCrossValidation(nrow(alcohol), 3, NULL, NULL)
> 
> # Sample code: Get cross-val predictions for main-effects only model
> alcohol$pred_add <- 0  # initialize the prediction vector
> for(i in 1:3) {
    split <- splitPlan[[i]]
    model_add <- lm(fmla_add, data = alcohol[split$train, ])
    alcohol$pred_add[split$app] <- predict(model_add, newdata = alcohol[split$app, ])
  }
> 
> # Get the cross-val predictions for the model with interactions
> alcohol$pred_interaction <- 0 # initialize the prediction vector
> for(i in 1:3) {
    split <- splitPlan[[i]]
    model_interaction <- lm(fmla_interaction, data = alcohol[split$train, ])
    alcohol$pred_interaction[split$app] <- predict(model_interaction, newdata = alcohol[split$app, ])
  }
> 
> # Get RMSE
> alcohol %>% 
    gather(key = modeltype, value = pred, pred_add, pred_interaction) %>%
    mutate(residuals = Metabol - pred) %>%
    group_by(modeltype) %>%
    summarize(rmse = sqrt(mean(residuals^2)))
# A tibble: 2 x 2
  modeltype         rmse
  <chr>            <dbl>
1 pred_add          1.64
2 pred_interaction  1.38
# fdata is in the workspace
> summary(fdata)
       y                 pred                      label   
 Min.   :  -5.894   Min.   :   1.072   small purchases:50  
 1st Qu.:   5.407   1st Qu.:   6.373   large purchases:50  
 Median :  57.374   Median :  55.693                       
 Mean   : 306.204   Mean   : 305.905                       
 3rd Qu.: 550.903   3rd Qu.: 547.886                       
 Max.   :1101.619   Max.   :1098.896
> 
> # Examine the data: generate the summaries for the groups large and small:
> fdata %>% 
      group_by(label) %>%     # group by small/large purchases
      summarize(min  = min(y),   # min of y
                mean = mean(y),   # mean of y
                max  = max(y))   # max of y
# A tibble: 2 x 4
  label             min   mean    max
  <fct>           <dbl>  <dbl>  <dbl>
1 small purchases -5.89   6.48   18.6
2 large purchases 96.1  606.   1102.
> 
> # Fill in the blanks to add error columns
> fdata2 <- fdata %>% 
           group_by(label)%>%       # group by label
             mutate(residual = y-pred,  # Residual
                    relerr   = residual/y)  # Relative error
> 
> # Compare the rmse and rmse.rel of the large and small groups:
> fdata2 %>% 
    group_by(label) %>% 
    summarize(rmse     = sqrt(mean(residual^2)),   # RMSE
              rmse.rel = sqrt(mean(relerr^2)))   # Root mean squared relative error
# A tibble: 2 x 3
  label            rmse rmse.rel
  <fct>           <dbl>    <dbl>
1 small purchases  4.01   1.25  
2 large purchases  5.54   0.0147
> 
> # Plot the predictions for both groups of purchases
> ggplot(fdata2, aes(x = pred, y = y, color = label)) + 
    geom_point() + 
    geom_abline() + 
    facet_wrap(~ label, ncol = 1, scales = "free") + 
    ggtitle("Outcome vs prediction")
> # Examine Income2005 in the training set
> summary(income_train$Income2005)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
     63   23000   39000   49894   61500  703637
> 
> # Write the formula for log income as a function of the tests and print it
> (fmla.log <- log(Income2005)~Arith+Word+Parag+Math+AFQT)
log(Income2005) ~ Arith + Word + Parag + Math + AFQT
> 
> # Fit the linear model
> model.log <-  lm(fmla.log,income_train)
> 
> # Make predictions on income_test
> income_test$logpred <- predict(model.log,income_test)
> summary(income_test$logpred)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  9.766  10.133  10.423  10.419  10.705  11.006
> 
> # Convert the predictions to monetary units
> income_test$pred.income <- exp(income_test$logpred)
> summary(income_test$pred.income)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  17432   25167   33615   35363   44566   60217
> 
> #  Plot predicted income (x axis) vs income
> ggplot(income_test, aes(x = pred.income, y = Income2005)) + 
    geom_point() + 
    geom_abline(color = "blue")
 > # fmla.abs is in the workspace
> fmla.abs
Income2005 ~ Arith + Word + Parag + Math + AFQT
> 
> # model.abs is in the workspace
> summary(model.abs)

Call:
lm(formula = fmla.abs, data = income_train)

Residuals:
   Min     1Q Median     3Q    Max 
-78728 -24137  -6979  11964 648573 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  17516.7     6420.1   2.728  0.00642 ** 
Arith         1552.3      303.4   5.116 3.41e-07 ***
Word          -132.3      265.0  -0.499  0.61754    
Parag        -1155.1      618.3  -1.868  0.06189 .  
Math           725.5      372.0   1.950  0.05127 .  
AFQT           177.8      144.1   1.234  0.21734    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 45500 on 2063 degrees of freedom
Multiple R-squared:  0.1165,	Adjusted R-squared:  0.1144 
F-statistic:  54.4 on 5 and 2063 DF,  p-value: < 2.2e-16
> 
> # Add predictions to the test set
> income_test <- income_test %>%
    mutate(pred.absmodel = predict(model.abs, income_test),        # predictions from model.abs
           pred.logmodel = exp(predict(model.log,income_test)))   # predictions from model.log
> 
> # Gather the predictions and calculate residuals and relative error
> income_long <- income_test %>% 
    gather(key = modeltype, value = pred, pred.absmodel, pred.logmodel) %>%
    mutate(residual = pred-Income2005,   # residuals
           relerr   = residual/Income2005)   # relative error
> 
> # Calculate RMSE and relative RMSE and compare
> income_long %>% 
    group_by(modeltype) %>%      # group by modeltype
    summarize(rmse     = sqrt(mean(residual^2)),    # RMSE
              rmse.rel = sqrt(mean((relerr )^2)))    # Root mean squared relative error
# A tibble: 2 x 3
  modeltype       rmse rmse.rel
  <chr>          <dbl>    <dbl>
1 pred.absmodel 37448.     3.18
2 pred.logmodel 39235.     2.22
> # houseprice is in the workspace
> summary(houseprice)
      size           price      
 Min.   : 44.0   Min.   : 42.0  
 1st Qu.: 73.5   1st Qu.:164.5  
 Median : 91.0   Median :203.5  
 Mean   : 94.3   Mean   :249.2  
 3rd Qu.:118.5   3rd Qu.:287.8  
 Max.   :150.0   Max.   :573.0
> 
> # Create the formula for price as a function of squared size
> (fmla_sqr <- price~I(size^2))
price ~ I(size^2)
> 
> # Fit a model of price as a function of squared size (use fmla_sqr)
> model_sqr <- lm(fmla_sqr,houseprice)
> 
> # Fit a model of price as a linear function of size
> model_lin <- lm(price~size,houseprice)
> 
> # Make predictions and compare
> houseprice %>% 
      mutate(pred_lin = predict(model_lin,houseprice),       # predictions from linear model
             pred_sqr = predict(model_sqr,houseprice)) %>%   # predictions from quadratic model 
      gather(key = modeltype, value = pred, pred_lin, pred_sqr) %>% # gather the predictions
      ggplot(aes(x = size)) + 
         geom_point(aes(y = price)) +                   # actual prices
         geom_line(aes(y = pred, color = modeltype)) + # the predictions
         scale_color_brewer(palette = "Dark2")
  > # houseprice is in the workspace
> summary(houseprice)
      size           price      
 Min.   : 44.0   Min.   : 42.0  
 1st Qu.: 73.5   1st Qu.:164.5  
 Median : 91.0   Median :203.5  
 Mean   : 94.3   Mean   :249.2  
 3rd Qu.:118.5   3rd Qu.:287.8  
 Max.   :150.0   Max.   :573.0
> 
> # fmla_sqr is in the workspace
> fmla_sqr
price ~ I(size^2)
> 
> # Create a splitting plan for 3-fold cross validation
> set.seed(34245)  # set the seed for reproducibility
> splitPlan <-  kWayCrossValidation(nrow(houseprice),3,NULL,NULL)
> 
> # Sample code: get cross-val predictions for price ~ size
> houseprice$pred_lin <- 0  # initialize the prediction vector
> for(i in 1:3) {
    split <- splitPlan[[i]]
    model_lin <- lm(price ~ size, data = houseprice[split$train,])
    houseprice$pred_lin[split$app] <- predict(model_lin, newdata = houseprice[split$app,])
  }
> 
> # Get cross-val predictions for price as a function of size^2 (use fmla_sqr)
> houseprice$pred_sqr <- 0 # initialize the prediction vector
> for(i in 1:3) {
    split <- splitPlan[[i]]
    model_sqr <- lm(fmla_sqr, data = houseprice[split$train, ])
    houseprice$pred_sqr[split$app] <- predict(model_sqr, newdata = houseprice[split$app, ])
  }
> 
> # Gather the predictions and calculate the residuals
> houseprice_long <- houseprice %>%
    gather(key = modeltype, value = pred, pred_lin, pred_sqr) %>%
    mutate(residuals = pred - price)
> 
> # Compare the cross-validated RMSE for the two models
> houseprice_long %>% 
    group_by(modeltype) %>% # group by modeltype
    summarize(rmse = sqrt(mean(residuals^2)))
# A tibble: 2 x 2
  modeltype  rmse
  <chr>     <dbl>
1 pred_lin   74.3
2 pred_sqr   63.7
#chap-4-Logistic regression to predict probabilities
 # sparrow is in the workspace
> summary(sparrow)
      status       age             total_length      wingspan    
 Perished:36   Length:87          Min.   :153.0   Min.   :236.0  
 Survived:51   Class :character   1st Qu.:158.0   1st Qu.:245.0  
               Mode  :character   Median :160.0   Median :247.0  
                                  Mean   :160.4   Mean   :247.5  
                                  3rd Qu.:162.5   3rd Qu.:251.0  
                                  Max.   :167.0   Max.   :256.0  
     weight       beak_head        humerus           femur       
 Min.   :23.2   Min.   :29.80   Min.   :0.6600   Min.   :0.6500  
 1st Qu.:24.7   1st Qu.:31.40   1st Qu.:0.7250   1st Qu.:0.7000  
 Median :25.8   Median :31.70   Median :0.7400   Median :0.7100  
 Mean   :25.8   Mean   :31.64   Mean   :0.7353   Mean   :0.7134  
 3rd Qu.:26.7   3rd Qu.:32.10   3rd Qu.:0.7500   3rd Qu.:0.7300  
 Max.   :31.0   Max.   :33.00   Max.   :0.7800   Max.   :0.7600  
    legbone          skull           sternum      
 Min.   :1.010   Min.   :0.5600   Min.   :0.7700  
 1st Qu.:1.110   1st Qu.:0.5900   1st Qu.:0.8300  
 Median :1.130   Median :0.6000   Median :0.8500  
 Mean   :1.131   Mean   :0.6032   Mean   :0.8511  
 3rd Qu.:1.160   3rd Qu.:0.6100   3rd Qu.:0.8800  
 Max.   :1.230   Max.   :0.6400   Max.   :0.9300
> 
> # Create the survived column
> sparrow$survived <-sparrow$status == "Survived"
> 
> # Create the formula
> (fmla <- survived~total_length+weight+humerus)
survived ~ total_length + weight + humerus
> 
> # Fit the logistic regression model
> sparrow_model <- glm(fmla,sparrow,family = binomial)
> 
> # Call summary
> summary(sparrow_model)

Call:
glm(formula = fmla, family = binomial, data = sparrow)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.1117  -0.6026   0.2871   0.6577   1.7082  

Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)   46.8813    16.9631   2.764 0.005715 ** 
total_length  -0.5435     0.1409  -3.858 0.000115 ***
weight        -0.5689     0.2771  -2.053 0.040060 *  
humerus       75.4610    19.1586   3.939 8.19e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 118.008  on 86  degrees of freedom
Residual deviance:  75.094  on 83  degrees of freedom
AIC: 83.094

Number of Fisher Scoring iterations: 5
> 
> # Call glance
> (perf <- glance(sparrow_model))
  null.deviance df.null    logLik      AIC      BIC deviance df.residual
1      118.0084      86 -37.54718 83.09436 92.95799 75.09436          83
> 
> # Calculate pseudo-R-squared
> (pseudoR2 <- 1-perf$deviance/perf$null.devianc)
[1] 0.3636526
> # sparrow is in the workspace
> summary(sparrow)
      status       age             total_length      wingspan    
 Perished:36   Length:87          Min.   :153.0   Min.   :236.0  
 Survived:51   Class :character   1st Qu.:158.0   1st Qu.:245.0  
               Mode  :character   Median :160.0   Median :247.0  
                                  Mean   :160.4   Mean   :247.5  
                                  3rd Qu.:162.5   3rd Qu.:251.0  
                                  Max.   :167.0   Max.   :256.0  
     weight       beak_head        humerus           femur       
 Min.   :23.2   Min.   :29.80   Min.   :0.6600   Min.   :0.6500  
 1st Qu.:24.7   1st Qu.:31.40   1st Qu.:0.7250   1st Qu.:0.7000  
 Median :25.8   Median :31.70   Median :0.7400   Median :0.7100  
 Mean   :25.8   Mean   :31.64   Mean   :0.7353   Mean   :0.7134  
 3rd Qu.:26.7   3rd Qu.:32.10   3rd Qu.:0.7500   3rd Qu.:0.7300  
 Max.   :31.0   Max.   :33.00   Max.   :0.7800   Max.   :0.7600  
    legbone          skull           sternum        survived      
 Min.   :1.010   Min.   :0.5600   Min.   :0.7700   Mode :logical  
 1st Qu.:1.110   1st Qu.:0.5900   1st Qu.:0.8300   FALSE:36       
 Median :1.130   Median :0.6000   Median :0.8500   TRUE :51       
 Mean   :1.131   Mean   :0.6032   Mean   :0.8511                  
 3rd Qu.:1.160   3rd Qu.:0.6100   3rd Qu.:0.8800                  
 Max.   :1.230   Max.   :0.6400   Max.   :0.9300
> 
> # sparrow_model is in the workspace
> summary(sparrow_model)

Call:
glm(formula = fmla, family = binomial, data = sparrow)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.1117  -0.6026   0.2871   0.6577   1.7082  

Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)   46.8813    16.9631   2.764 0.005715 ** 
total_length  -0.5435     0.1409  -3.858 0.000115 ***
weight        -0.5689     0.2771  -2.053 0.040060 *  
humerus       75.4610    19.1586   3.939 8.19e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 118.008  on 86  degrees of freedom
Residual deviance:  75.094  on 83  degrees of freedom
AIC: 83.094

Number of Fisher Scoring iterations: 5
> 
> # Make predictions
> sparrow$pred <- predict(sparrow_model, type = "response")
> 
> # Look at gain curve
> GainCurvePlot(sparrow, "pred", "survived", "sparrow survival model")
Warning message: Using `intercept` and/or `slope` with `mapping` may not have the desired result as mapping is overwritten if either of these is specified
> # bikesJuly is in the workspace
> str(bikesJuly)
'data.frame':	744 obs. of  12 variables:
 $ hr        : Factor w/ 24 levels "0","1","2","3",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ holiday   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ workingday: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ weathersit: chr  "Clear to partly cloudy" "Clear to partly cloudy" "Clear to partly cloudy" "Clear to partly cloudy" ...
 $ temp      : num  0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ...
 $ atemp     : num  0.727 0.697 0.697 0.712 0.667 ...
 $ hum       : num  0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ...
 $ windspeed : num  0 0.1343 0.0896 0.1343 0.194 ...
 $ cnt       : int  149 93 90 33 4 10 27 50 142 219 ...
 $ instant   : int  13004 13005 13006 13007 13008 13009 13010 13011 13012 13013 ...
 $ mnth      : int  7 7 7 7 7 7 7 7 7 7 ...
 $ yr        : int  1 1 1 1 1 1 1 1 1 1 ...
> 
> # The outcome column
> outcome
[1] "cnt"
> 
> # The inputs to use
> vars
[1] "hr"         "holiday"    "workingday" "weathersit" "temp"      
[6] "atemp"      "hum"        "windspeed"
> 
> # Create the formula string for bikes rented as a function of the inputs
> (fmla <- paste(outcome , "~", paste(vars, collapse = " + ")))
[1] "cnt ~ hr + holiday + workingday + weathersit + temp + atemp + hum + windspeed"
> 
> # Calculate the mean and variance of the outcome
> (mean_bikes <- mean(bikesJuly$cnt))
[1] 273.6653
> (var_bikes <- var(bikesJuly$cnt))
[1] 45863.84
> 
> # Fit the model
> bike_model <- glm(fmla,bikesJuly,family = quasipoisson)
> 
> # Call glance
> (perf <- glance(bike_model))
  null.deviance df.null logLik AIC BIC deviance df.residual
1      133364.9     743     NA  NA  NA  28774.9         712
> 
> # Calculate pseudo-R-squared
> (pseudoR2 <- 1-perf$deviance /perf$null.deviance)
[1] 0.7842393
> # bikesAugust is in the workspace
> str(bikesAugust)
'data.frame':	744 obs. of  12 variables:
 $ hr        : Factor w/ 24 levels "0","1","2","3",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ holiday   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ workingday: logi  TRUE TRUE TRUE TRUE TRUE TRUE ...
 $ weathersit: chr  "Clear to partly cloudy" "Clear to partly cloudy" "Clear to partly cloudy" "Clear to partly cloudy" ...
 $ temp      : num  0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 ...
 $ atemp     : num  0.636 0.606 0.576 0.576 0.591 ...
 $ hum       : num  0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 ...
 $ windspeed : num  0.1642 0.0896 0.1045 0.1045 0.1343 ...
 $ cnt       : int  47 33 13 7 4 49 185 487 681 350 ...
 $ instant   : int  13748 13749 13750 13751 13752 13753 13754 13755 13756 13757 ...
 $ mnth      : int  8 8 8 8 8 8 8 8 8 8 ...
 $ yr        : int  1 1 1 1 1 1 1 1 1 1 ...
> 
> # bike_model is in the workspace
> summary(bike_model)

Call:
glm(formula = fmla, family = quasipoisson, data = bikesJuly)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-21.6117   -4.3121   -0.7223    3.5507   16.5079  

Coefficients:
                               Estimate Std. Error t value Pr(>|t|)    
(Intercept)                    5.934986   0.439027  13.519  < 2e-16 ***
hr1                           -0.580055   0.193354  -3.000 0.002794 ** 
hr2                           -0.892314   0.215452  -4.142 3.86e-05 ***
hr3                           -1.662342   0.290658  -5.719 1.58e-08 ***
hr4                           -2.350204   0.393560  -5.972 3.71e-09 ***
hr5                           -1.084289   0.230130  -4.712 2.96e-06 ***
hr6                            0.211945   0.156476   1.354 0.176012    
hr7                            1.211135   0.132332   9.152  < 2e-16 ***
hr8                            1.648361   0.127177  12.961  < 2e-16 ***
hr9                            1.155669   0.133927   8.629  < 2e-16 ***
hr10                           0.993913   0.137096   7.250 1.09e-12 ***
hr11                           1.116547   0.136300   8.192 1.19e-15 ***
hr12                           1.282685   0.134769   9.518  < 2e-16 ***
hr13                           1.273010   0.135872   9.369  < 2e-16 ***
hr14                           1.237721   0.136386   9.075  < 2e-16 ***
hr15                           1.260647   0.136144   9.260  < 2e-16 ***
hr16                           1.515893   0.132727  11.421  < 2e-16 ***
hr17                           1.948404   0.128080  15.212  < 2e-16 ***
hr18                           1.893915   0.127812  14.818  < 2e-16 ***
hr19                           1.669277   0.128471  12.993  < 2e-16 ***
hr20                           1.420732   0.131004  10.845  < 2e-16 ***
hr21                           1.146763   0.134042   8.555  < 2e-16 ***
hr22                           0.856182   0.138982   6.160 1.21e-09 ***
hr23                           0.479197   0.148051   3.237 0.001265 ** 
holidayTRUE                    0.201598   0.079039   2.551 0.010961 *  
workingdayTRUE                 0.116798   0.033510   3.485 0.000521 ***
weathersitLight Precipitation -0.214801   0.072699  -2.955 0.003233 ** 
weathersitMisty               -0.010757   0.038600  -0.279 0.780572    
temp                          -3.246001   1.148270  -2.827 0.004833 ** 
atemp                          2.042314   0.953772   2.141 0.032589 *  
hum                           -0.748557   0.236015  -3.172 0.001581 ** 
windspeed                      0.003277   0.148814   0.022 0.982439    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for quasipoisson family taken to be 38.98949)

    Null deviance: 133365  on 743  degrees of freedom
Residual deviance:  28775  on 712  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 5
> 
> # Make predictions on August data
> bikesAugust$pred  <- predict(bike_model,newdata = bikesAugust,type = "response")
> 
> # Calculate the RMSE
> bikesAugust %>% 
    mutate(residual = pred-cnt) %>%
    summarize(rmse  = sqrt(mean(residual^2)))
      rmse
1 112.5815
> 
> # Plot predictions vs cnt (pred on x-axis)
> ggplot(bikesAugust, aes(x = pred, y = cnt)) +
    geom_point() + 
    geom_abline(color = "darkblue")
  # Plot predictions and cnt by date/time
> bikesAugust %>% 
    # set start to 0, convert unit to days
    mutate(instant = (instant - min(instant))/24) %>%  
    # gather cnt and pred into a value column
    gather(key = valuetype, value = value, cnt, pred) %>%
    filter(instant < 14) %>% # restric to first 14 days
    # plot value by instant
    ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + 
    geom_point() + 
    geom_line() + 
    scale_x_continuous("Day", breaks = 0:14, labels = 0:14) + 
    scale_color_brewer(palette = "Dark2") + 
    ggtitle("Predicted August bike rentals, Quasipoisson model")
   # soybean_train is in the workspace
summary(soybean_train)

# Plot weight vs Time (Time on x axis)
ggplot(soybean_train, aes(x = Time, y = weight)) + 
  geom_point() 
  # soybean_test is in the workspace
summary(soybean_test)

# Get predictions from linear model
soybean_test$pred.lin <- predict(model.lin, newdata = soybean_test)

# Get predictions from gam model
soybean_test$pred.gam <- as.numeric(predict(model.gam, newdata = soybean_test))

# Gather the predictions into a "long" dataset
soybean_long <- soybean_test %>%
  gather(key = modeltype, value = pred, pred.lin, pred.gam)
 > # soybean_test is in the workspace
> summary(soybean_test)
      Plot    Variety   Year         Time           weight       
 1988F8 : 4   F:43    1988:32   Min.   :14.00   Min.   : 0.0380  
 1988P7 : 4   P:39    1989:26   1st Qu.:23.00   1st Qu.: 0.4248  
 1989F8 : 4           1990:24   Median :41.00   Median : 3.0025  
 1990F8 : 4                     Mean   :44.09   Mean   : 7.1576  
 1988F4 : 3                     3rd Qu.:69.00   3rd Qu.:15.0113  
 1988F2 : 3                     Max.   :84.00   Max.   :30.2717  
 (Other):60
> 
> # Get predictions from linear model
> soybean_test$pred.lin <- predict(model.lin, newdata = soybean_test)
> 
> # Get predictions from gam model
> soybean_test$pred.gam <- as.numeric(predict(model.gam, newdata = soybean_test))
> 
> # Gather the predictions into a "long" dataset
> soybean_long <- soybean_test %>%
    gather(key = modeltype, value = pred, pred.lin, pred.gam)
> 
> # Calculate the rmse
> soybean_long %>%
    mutate(residual = weight - pred) %>%     # residuals
    group_by(modeltype) %>%                  # group by modeltype
    summarize(rmse = sqrt(mean(residual^2))) # calculate the RMSE
# A tibble: 2 x 2
  modeltype  rmse
  <chr>     <dbl>
1 pred.gam   2.29
2 pred.lin   3.19
> 
> # Compare the predictions against actual weights on the test data
> soybean_long %>%
    ggplot(aes(x = Time)) +                          # the column for the x axis
    geom_point(aes(y = weight)) +                    # the y-column for the scatterplot
    geom_point(aes(y = pred, color = modeltype)) +   # the y-column for the point-and-line plot
    geom_line(aes(y = pred, color = modeltype, linetype = modeltype)) + # the y-column for the point-and-line plot
    scale_color_brewer(palette = "Dark2")
 ##chap-5-tree-based methods
 > # bikesJuly is in the workspace
> str(bikesJuly)
'data.frame':	744 obs. of  12 variables:
 $ hr        : Factor w/ 24 levels "0","1","2","3",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ holiday   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ workingday: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ weathersit: chr  "Clear to partly cloudy" "Clear to partly cloudy" "Clear to partly cloudy" "Clear to partly cloudy" ...
 $ temp      : num  0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ...
 $ atemp     : num  0.727 0.697 0.697 0.712 0.667 ...
 $ hum       : num  0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ...
 $ windspeed : num  0 0.1343 0.0896 0.1343 0.194 ...
 $ cnt       : int  149 93 90 33 4 10 27 50 142 219 ...
 $ instant   : int  13004 13005 13006 13007 13008 13009 13010 13011 13012 13013 ...
 $ mnth      : int  7 7 7 7 7 7 7 7 7 7 ...
 $ yr        : int  1 1 1 1 1 1 1 1 1 1 ...
> 
> # Random seed to reproduce results
> seed
[1] 423563
> 
> # The outcome column
> (outcome <- "cnt")
[1] "cnt"
> 
> # The input variables
> (vars <- c("hr", "holiday", "workingday", "weathersit", "temp", "atemp", "hum", "windspeed"))
[1] "hr"         "holiday"    "workingday" "weathersit" "temp"      
[6] "atemp"      "hum"        "windspeed"
> 
> # Create the formula string for bikes rented as a function of the inputs
> (fmla <- paste(outcome, "~", paste(vars, collapse = " + ")))
[1] "cnt ~ hr + holiday + workingday + weathersit + temp + atemp + hum + windspeed"
> 
> # Load the package ranger
> library(ranger)
> 
> # Fit and print the random forest model
> (bike_model_rf <- ranger(fmla, # formula 
                           bikesJuly, # data
                           num.trees = 500, 
                           respect.unordered.factors = "order", 
                           seed = seed))
Ranger result

Call:
 ranger(fmla, bikesJuly, num.trees = 500, respect.unordered.factors = "order",      seed = seed) 

Type:                             Regression 
Number of trees:                  500 
Sample size:                      744 
Number of independent variables:  8 
Mtry:                             2 
Target node size:                 5 
Variable importance mode:         none 
Splitrule:                        variance 
OOB prediction error (MSE):       8230.568 
R squared (OOB):                  0.8205434
> # bikesAugust is in the workspace
> str(bikesAugust)
'data.frame':	744 obs. of  12 variables:
 $ hr        : Factor w/ 24 levels "0","1","2","3",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ holiday   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ workingday: logi  TRUE TRUE TRUE TRUE TRUE TRUE ...
 $ weathersit: chr  "Clear to partly cloudy" "Clear to partly cloudy" "Clear to partly cloudy" "Clear to partly cloudy" ...
 $ temp      : num  0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 ...
 $ atemp     : num  0.636 0.606 0.576 0.576 0.591 ...
 $ hum       : num  0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 ...
 $ windspeed : num  0.1642 0.0896 0.1045 0.1045 0.1343 ...
 $ cnt       : int  47 33 13 7 4 49 185 487 681 350 ...
 $ instant   : int  13748 13749 13750 13751 13752 13753 13754 13755 13756 13757 ...
 $ mnth      : int  8 8 8 8 8 8 8 8 8 8 ...
 $ yr        : int  1 1 1 1 1 1 1 1 1 1 ...
> 
> # bike_model_rf is in the workspace
> bike_model_rf
Ranger result

Call:
 ranger(fmla, bikesJuly, num.trees = 500, respect.unordered.factors = "order",      seed = seed) 

Type:                             Regression 
Number of trees:                  500 
Sample size:                      744 
Number of independent variables:  8 
Mtry:                             2 
Target node size:                 5 
Variable importance mode:         none 
Splitrule:                        variance 
OOB prediction error (MSE):       8299.851 
R squared (OOB):                  0.8190328
> 
> # Make predictions on the August data
> bikesAugust$pred <- predict(bike_model_rf, bikesAugust)$predictions
> 
> # Calculate the RMSE of the predictions
> bikesAugust %>% 
    mutate(residual = cnt - pred)  %>%        # calculate the residual
    summarize(rmse  = sqrt(mean(residual^2))) # calculate rmse
      rmse
1 96.66032
> 
> # Plot actual outcome vs predictions (predictions on x-axis)
> ggplot(bikesAugust, aes(x = pred, y = cnt)) + 
    geom_point() + 
    geom_abline()
    
  > first_two_weeks <- bikesAugust %>% 
    # Set start to 0, convert unit to days
    mutate(instant = (instant - min(instant)) / 24) %>% 
    # Gather cnt and pred into a column named value with key valuetype
  gather(key = valuetype, value = value, cnt, pred) %>%
    # Filter for rows in the first two
    filter(instant < 14)
> 
> # Plot predictions and cnt by date/time
> ggplot(first_two_weeks, aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + 
    geom_point() + 
    geom_line() + 
    scale_x_continuous("Day", breaks = 0:14, labels = 0:14) + 
    scale_color_brewer(palette = "Dark2") + 
    ggtitle("Predicted August bike rentals, Random Forest plot")
>> # dframe is in the workspace
> dframe
   color size popularity
1      b   13  1.0785088
2      r   11  1.3956245
3      r   15  0.9217988
4      r   14  1.2025453
5      r   13  1.0838662
6      b   11  0.8043527
7      r    9  1.1035440
8      g   12  0.8746332
9      b    7  0.6947058
10     b   12  0.8832502
> 
> # Create a vector of variable names
> (vars <- c("color", "size"))
[1] "color" "size"
> 
> # Load the package vtreat
> library(vtreat)
> 
> # Create the treatment plan
> treatplan <- designTreatmentsZ(dframe, vars)
[1] "vtreat 1.2.0 inspecting inputs Thu Nov 21 08:51:34 2019"
[1] "designing treatments Thu Nov 21 08:51:34 2019"
[1] " have initial level statistics Thu Nov 21 08:51:34 2019"
[1] "design var color Thu Nov 21 08:51:34 2019"
[1] "design var size Thu Nov 21 08:51:34 2019"
[1] " scoring treatments Thu Nov 21 08:51:34 2019"
[1] "have treatment plan Thu Nov 21 08:51:34 2019"
> 
> # Examine the scoreFrame
> (scoreFrame <- treatplan %>%
      use_series(scoreFrame) %>%
      select(varName, origName, code))
        varName origName  code
1    color_catP    color  catP
2    size_clean     size clean
3 color_lev_x_b    color   lev
4 color_lev_x_g    color   lev
5 color_lev_x_r    color   lev
> 
> # We only want the rows with codes "clean" or "lev"
> (newvars <- scoreFrame %>%
      filter(code %in% c("clean", "lev")) %>%
      use_series(varName))
[1] "size_clean"    "color_lev_x_b" "color_lev_x_g" "color_lev_x_r"
> 
> # Create the treated training data
> (dframe.treat <- prepare(treatplan, dframe, varRestriction = newvars))
   size_clean color_lev_x_b color_lev_x_g color_lev_x_r
1          13             1             0             0
2          11             0             0             1
3          15             0             0             1
4          14             0             0             1
5          13             0             0             1
6          11             1             0             0
7           9             0             0             1
8          12             0             1             0
9           7             1             0             0
10         12             1             0             0
> > # treatplan is in the workspace
> summary(treatplan)
              Length Class           Mode     
treatments    3      -none-          list     
scoreFrame    8      data.frame      list     
outcomename   1      -none-          character
vtreatVersion 1      package_version list     
outcomeType   1      -none-          character
outcomeTarget 1      -none-          character
meanY         1      -none-          logical  
splitmethod   1      -none-          character
> 
> # newvars is in the workspace
> newvars
[1] "size_clean"    "color_lev_x_b" "color_lev_x_g" "color_lev_x_r"
> 
> # Print dframe and testframe
> dframe
   color size popularity
1      b   13  1.0785088
2      r   11  1.3956245
3      r   15  0.9217988
4      r   14  1.2025453
5      r   13  1.0838662
6      b   11  0.8043527
7      r    9  1.1035440
8      g   12  0.8746332
9      b    7  0.6947058
10     b   12  0.8832502
> testframe
   color size popularity
1      g    7  0.9733920
2      g    8  0.9122529
3      y   10  1.4217153
4      g   12  1.1905828
5      g    6  0.9866464
6      y    8  1.3697515
7      b   12  1.0959387
8      g   12  0.9161547
9      g   12  1.0000460
10     r    8  1.3137360
> 
> # Use prepare() to one-hot-encode testframe
> (testframe.treat <-prepare(treatplan, testframe, varRestriction = newvars))
   size_clean color_lev_x_b color_lev_x_g color_lev_x_r
1           7             0             1             0
2           8             0             1             0
3          10             0             0             0
4          12             0             1             0
5           6             0             1             0
6           8             0             0             0
7          12             1             0             0
8          12             0             1             0
9          12             0             1             0
10          8             0             0             1
> > # treatplan is in the workspace
> summary(treatplan)
              Length Class           Mode     
treatments    3      -none-          list     
scoreFrame    8      data.frame      list     
outcomename   1      -none-          character
vtreatVersion 1      package_version list     
outcomeType   1      -none-          character
outcomeTarget 1      -none-          character
meanY         1      -none-          logical  
splitmethod   1      -none-          character
> 
> # newvars is in the workspace
> newvars
[1] "size_clean"    "color_lev_x_b" "color_lev_x_g" "color_lev_x_r"
> 
> # Print dframe and testframe
> dframe
   color size popularity
1      b   13  1.0785088
2      r   11  1.3956245
3      r   15  0.9217988
4      r   14  1.2025453
5      r   13  1.0838662
6      b   11  0.8043527
7      r    9  1.1035440
8      g   12  0.8746332
9      b    7  0.6947058
10     b   12  0.8832502
> testframe
   color size popularity
1      g    7  0.9733920
2      g    8  0.9122529
3      y   10  1.4217153
4      g   12  1.1905828
5      g    6  0.9866464
6      y    8  1.3697515
7      b   12  1.0959387
8      g   12  0.9161547
9      g   12  1.0000460
10     r    8  1.3137360
> 
> # Use prepare() to one-hot-encode testframe
> (testframe.treat <-prepare(treatplan, testframe, varRestriction = newvars))
   size_clean color_lev_x_b color_lev_x_g color_lev_x_r
1           7             0             1             0
2           8             0             1             0
3          10             0             0             0
4          12             0             1             0
5           6             0             1             0
6           8             0             0             0
7          12             1             0             0
8          12             0             1             0
9          12             0             1             0
10          8             0             0             1
> > # The outcome column
> (outcome <- "cnt")
[1] "cnt"
> 
> # The input columns
> (vars <- c("hr", "holiday", "workingday", "weathersit", "temp", "atemp", "hum", "windspeed"))
[1] "hr"         "holiday"    "workingday" "weathersit" "temp"      
[6] "atemp"      "hum"        "windspeed"
> 
> # Load the package vtreat
> library(vtreat)
> 
> # Create the treatment plan from bikesJuly (the training data)
> treatplan <- designTreatmentsZ(bikesJuly, vars, verbose = FALSE)
> 
> # Get the "clean" and "lev" variables from the scoreFrame
> (newvars <- treatplan %>%
    use_series(scoreFrame) %>%               
    filter(code %in% c("clean", "lev")) %>%  # get the variables you care about
    use_series(varName))                     # get the varName column
 [1] "holiday_clean"                          
 [2] "workingday_clean"                       
 [3] "temp_clean"                             
 [4] "atemp_clean"                            
 [5] "hum_clean"                              
 [6] "windspeed_clean"                        
 [7] "hr_lev_x_0"                             
 [8] "hr_lev_x_1"                             
 [9] "hr_lev_x_10"                            
[10] "hr_lev_x_11"                            
[11] "hr_lev_x_12"                            
[12] "hr_lev_x_13"                            
[13] "hr_lev_x_14"                            
[14] "hr_lev_x_15"                            
[15] "hr_lev_x_16"                            
[16] "hr_lev_x_17"                            
[17] "hr_lev_x_18"                            
[18] "hr_lev_x_19"                            
[19] "hr_lev_x_2"                             
[20] "hr_lev_x_20"                            
[21] "hr_lev_x_21"                            
[22] "hr_lev_x_22"                            
[23] "hr_lev_x_23"                            
[24] "hr_lev_x_3"                             
[25] "hr_lev_x_4"                             
[26] "hr_lev_x_5"                             
[27] "hr_lev_x_6"                             
[28] "hr_lev_x_7"                             
[29] "hr_lev_x_8"                             
[30] "hr_lev_x_9"                             
[31] "weathersit_lev_x_Clear_to_partly_cloudy"
[32] "weathersit_lev_x_Light_Precipitation"   
[33] "weathersit_lev_x_Misty"
> 
> # Prepare the training data
> bikesJuly.treat <- prepare(treatplan, bikesJuly,  varRestriction = newvars)
> 
> # Prepare the test data
> bikesAugust.treat <- prepare(treatplan, bikesAugust, varRestriction = newvars)
> 
> # Call str() on the treated data
> str(bikesJuly.treat)
'data.frame':	744 obs. of  33 variables:
 $ holiday_clean                          : num  0 0 0 0 0 0 0 0 0 0 ...
 $ workingday_clean                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ temp_clean                             : num  0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ...
 $ atemp_clean                            : num  0.727 0.697 0.697 0.712 0.667 ...
 $ hum_clean                              : num  0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ...
 $ windspeed_clean                        : num  0 0.1343 0.0896 0.1343 0.194 ...
 $ hr_lev_x_0                             : num  1 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_1                             : num  0 1 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_10                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_11                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_12                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_13                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_14                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_15                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_16                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_17                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_18                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_19                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_2                             : num  0 0 1 0 0 0 0 0 0 0 ...
 $ hr_lev_x_20                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_21                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_22                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_23                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_3                             : num  0 0 0 1 0 0 0 0 0 0 ...
 $ hr_lev_x_4                             : num  0 0 0 0 1 0 0 0 0 0 ...
 $ hr_lev_x_5                             : num  0 0 0 0 0 1 0 0 0 0 ...
 $ hr_lev_x_6                             : num  0 0 0 0 0 0 1 0 0 0 ...
 $ hr_lev_x_7                             : num  0 0 0 0 0 0 0 1 0 0 ...
 $ hr_lev_x_8                             : num  0 0 0 0 0 0 0 0 1 0 ...
 $ hr_lev_x_9                             : num  0 0 0 0 0 0 0 0 0 1 ...
 $ weathersit_lev_x_Clear_to_partly_cloudy: num  1 1 1 1 1 1 1 1 1 1 ...
 $ weathersit_lev_x_Light_Precipitation   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ weathersit_lev_x_Misty                 : num  0 0 0 0 0 0 0 0 0 0 ...
> str(bikesAugust.treat)
'data.frame':	744 obs. of  33 variables:
 $ holiday_clean                          : num  0 0 0 0 0 0 0 0 0 0 ...
 $ workingday_clean                       : num  1 1 1 1 1 1 1 1 1 1 ...
 $ temp_clean                             : num  0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 ...
 $ atemp_clean                            : num  0.636 0.606 0.576 0.576 0.591 ...
 $ hum_clean                              : num  0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 ...
 $ windspeed_clean                        : num  0.1642 0.0896 0.1045 0.1045 0.1343 ...
 $ hr_lev_x_0                             : num  1 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_1                             : num  0 1 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_10                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_11                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_12                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_13                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_14                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_15                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_16                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_17                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_18                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_19                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_2                             : num  0 0 1 0 0 0 0 0 0 0 ...
 $ hr_lev_x_20                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_21                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_22                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_23                            : num  0 0 0 0 0 0 0 0 0 0 ...
 $ hr_lev_x_3                             : num  0 0 0 1 0 0 0 0 0 0 ...
 $ hr_lev_x_4                             : num  0 0 0 0 1 0 0 0 0 0 ...
 $ hr_lev_x_5                             : num  0 0 0 0 0 1 0 0 0 0 ...
 $ hr_lev_x_6                             : num  0 0 0 0 0 0 1 0 0 0 ...
 $ hr_lev_x_7                             : num  0 0 0 0 0 0 0 1 0 0 ...
 $ hr_lev_x_8                             : num  0 0 0 0 0 0 0 0 1 0 ...
 $ hr_lev_x_9                             : num  0 0 0 0 0 0 0 0 0 1 ...
 $ weathersit_lev_x_Clear_to_partly_cloudy: num  1 1 1 1 0 0 1 0 0 0 ...
 $ weathersit_lev_x_Light_Precipitation   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ > # The July data is in the workspace
> ls()
[1] "bikesJuly"       "bikesJuly.treat"
> 
> # Load the package xgboost
> library(xgboost)

Attaching package: 'xgboost'
The following object is masked from 'package:dplyr':

    slice
> 
> # Run xgb.cv
> cv <- xgb.cv(data = as.matrix(bikesJuly.treat), 
              label = bikesJuly$cnt,
              nrounds = 100,
              nfold = 5,
              objective = "reg:linear",
              eta = 0.3,
              max_depth = 6,
              early_stopping_rounds = 10,
              verbose = 0    # silent
  )
> 
> # Get the evaluation log
> elog <- cv$evaluation_log
> 
> # Determine and print how many trees minimize training and test error
> elog %>% 
     summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min(train_rmse_mean)
               ntrees.test  = which.min(test_rmse_mean))   # find the index of min(test_rmse_mean)
  ntrees.train ntrees.test
1           75          65 weathersit_lev_x_Misty                 : num  0 0 0 0 1 1 0 1 1 1 ...
> > # Examine the workspace
> ls()
[1] "bikesAugust"       "bikesAugust.treat" "bikesJuly"        
[4] "bikesJuly.treat"   "ntrees"
> 
> # The number of trees to use, as determined by xgb.cv
> ntrees
[1] 84
> 
> # Run xgboost
> bike_model_xgb <- xgboost(data = as.matrix(bikesJuly.treat), # training data as matrix
                     label = bikesJuly$cnt,  # column of outcomes
                     nrounds = ntrees,       # number of trees to build
                     objective = "reg:linear", # objective
                     eta = 0.3,
                     depth = 6,
                     verbose = 0  # silent
  )
> 
> # Make predictions
> bikesAugust$pred <-  predict(bike_model_xgb, as.matrix(bikesAugust.treat))
> 
> # Plot predictions (on x axis) vs actual bike rental count
> ggplot(bikesAugust, aes(x = pred, y = cnt)) + 
    geom_point() + 
    geom_abline()
> > # bikesAugust is in the workspace
> str(bikesAugust)
'data.frame':	744 obs. of  13 variables:
 $ hr        : Factor w/ 24 levels "0","1","2","3",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ holiday   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ workingday: logi  TRUE TRUE TRUE TRUE TRUE TRUE ...
 $ weathersit: chr  "Clear to partly cloudy" "Clear to partly cloudy" "Clear to partly cloudy" "Clear to partly cloudy" ...
 $ temp      : num  0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 ...
 $ atemp     : num  0.636 0.606 0.576 0.576 0.591 ...
 $ hum       : num  0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 ...
 $ windspeed : num  0.1642 0.0896 0.1045 0.1045 0.1343 ...
 $ cnt       : int  47 33 13 7 4 49 185 487 681 350 ...
 $ instant   : int  13748 13749 13750 13751 13752 13753 13754 13755 13756 13757 ...
 $ mnth      : int  8 8 8 8 8 8 8 8 8 8 ...
 $ yr        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ pred      : num  47.208 37.05 0.342 -6.934 3.104 ...
> 
> # Calculate RMSE
> bikesAugust %>%
    mutate(residuals = cnt - pred) %>%
    summarize(rmse = sqrt(mean(residuals^2)))
      rmse
1 75.90971
>> # Print quasipoisson_plot
> quasipoisson_plot
> 
> # Print randomforest_plot
> randomforest_plot
> 
> # Plot predictions and actual bike rentals as a function of time (days)
> bikesAugust %>% 
    mutate(instant = (instant - min(instant))/24) %>%  # set start to 0, convert unit to days
    gather(key = valuetype, value = value, cnt, pred) %>%
    filter(instant < 14) %>% # first two weeks
    ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + 
    geom_point() + 
    geom_line() + 
    scale_x_continuous("Day", breaks = 0:14, labels = 0:14) + 
    scale_color_brewer(palette = "Dark2") + 
     ggtitle("Predicted August bike rentals, Gradient Boosting model")
> 
