##6 Cluster Analysis in R
#chapter1-What is cluster analysis?
> # Plot the positions of the players
> ggplot(two_players, aes(x = x, y = y)) + 
    geom_point() +
    # Assuming a 40x60 field
    lims(x = c(-30,30), y = c(-20, 20))
> 
> # Split the players data frame into two observations
> player1 <- two_players[1, ]
> player2 <- two_players[2, ]
> 
> # Calculate and print their distance using the Euclidean Distance formula
> player_distance <- sqrt( (player1$x - player2$x)^2 + (player1$y - player2$y)^2 )
> player_distance
[1] 11.6619
> # Calculate the Distance Between two_players
> dist_two_players <-dist(two_players)
> dist_two_players
        1
2 11.6619
> 
> # Calculate the Distance Between three_players
> dist_three_players <- dist(three_players)
> dist_three_players
         1        2
2 11.66190         
3 16.76305 18.02776
> Who are the closest players?
You are given the data frame containing the positions of 4 players on a soccer field.

This data is preloaded as four_players in your environment and is displayed below.

Player	x	y
1	5	4
2	15	10
3	0	20
4	-5	5


Work in the R console to answer the following question:

Which two players are closest to one another?

Instructions
50 XP
Possible Answers
1 & 2
1 & 3
1 & 4
2 & 3
2 & 4
3 & 4
Not enough information to decide
Effects of scale
You have learned that when a variable is on a larger scale than other variables in your data it may disproportionately influence the resulting distance calculated between your observations. Lets see this in action by observing a sample of data from the trees data set.

You will leverage the scale() function which by default centers & scales our column features.

Our variables are the following:

Girth - tree diameter in inches
Height - tree height in inches
> # Calculate distance for three_trees
> dist_trees <- dist(three_trees)
> 
> # Scale three trees & calculate the distance
> scaled_three_trees <- scale(three_trees )
> dist_scaled_trees <- dist(scaled_three_trees)
> 
> # Output the results of both Matrices
> print('Without Scaling')
[1] "Without Scaling"
> dist_trees
         1        2
2 60.00075         
3 24.10062 84.02149
> 
> print('With Scaling')
[1] "With Scaling"
> dist_scaled_trees
         1        2
2 1.409365         
3 1.925659 2.511082
> Calculating distance between categorical variables
In this exercise you will explore how to calculate binary (Jaccard) distances. In order to calculate distances we will first have to dummify our categories using the dummy.data.frame() from the library dummies

You will use a small collection of survey observations stored in the data frame job_survey with the following columns:

job_satisfaction Possible options: "Hi", "Mid", "Low"
is_happy Possible options: "Yes", "No"
> # Dummify the Survey Data
> dummy_survey <- dummy.data.frame(job_survey)
> 
> # Calculate the Distance
> dist_survey <- dist(dummy_survey, method="binary")
> 
> # Print the Original Data
> job_survey
  job_satisfaction is_happy
1              Low       No
2              Low       No
3               Hi      Yes
4              Low       No
5              Mid       No
> 
> # Print the Distance Matrix
> dist_survey
          1         2         3         4
2 0.0000000                              
3 1.0000000 1.0000000                    
4 0.0000000 0.0000000 1.0000000          
5 0.6666667 0.6666667 1.0000000 0.6666667
> #chapter2-Hierarchical clustering
> # Extract the pair distances
> distance_1_2 <- dist_players[1]
> distance_1_3 <- dist_players[2]
> distance_2_3 <- dist_players[3]
> 
> # Calculate the complete distance between group 1-2 and 3
> complete <- max(c(distance_1_3, distance_2_3))
> complete
[1] 18.02776
> 
> # Calculate the single distance between group 1-2 and 3
> single <- min(c(distance_1_3, distance_2_3))
> single
[1] 16.76305
> 
> # Calculate the average distance between group 1-2 and 3
> average <- mean(c(distance_1_3, distance_2_3))
> average
[1] 17.39541
> Revisited: The closest observation to a pair
You are now ready to answer this question!

Below you see a pre-calculated distance matrix between four players on a soccer field. You can clearly see that players 1 & 4 are the closest to one another with a Euclidean distance value of 10. This distance matrix is available for your exploration as the variable dist_players

1	2	3
2	11.7		
3	16.8	18.0	
4	10.0	20.6	15.8

If 1 and 4 are the closest players among the four, which player is closest to players 1 and 4?

Instructions
50 XP
Possible Answers
Complete Linkage: Player 3,
Single & Average Linkage: Player 2
Complete Linkage: Player 2,
Single & Average Linkage: Player 3
Player 2 using Complete, Single & Average Linkage methods
Player 3 using Complete, Single & Average Linkage methods
Assign cluster membership
In this exercise you will leverage the hclust() function to calculate the iterative linkage steps and you will use the cutree() function to extract the cluster assignments for the desired number (k) of clusters.

You are given the positions of 12 players at the start of a 6v6 soccer match. This is stored in the lineup data frame.

You know that this match has two teams (k = 2), let's use the clustering methods you learned to assign which team each player belongs in based on their position.

Notes:

The linkage method can be passed via the method parameter: hclust(distance_matrix, method = "complete")
Remember that in soccer opposing teams start on their half of the field.
Because these positions are measured using the same scale we do not need to re-scale our data.
# Calculate the Distance
> dist_players <- dist(lineup)
> 
> # Perform the hierarchical clustering using the complete linkage
> hc_players <- hclust(dist_players,method="complete")
> 
> # Calculate the assignment vector with a k of 2
> clusters_k2 <- cutree(hc_players,k=2)
> 
> # Create a new data frame storing these results
> lineup_k2_complete <- mutate(lineup, cluster =clusters_k2)
> Exploring the clusters
Because clustering analysis is always in part qualitative, it is incredibly important to have the necessary tools to explore the results of the clustering.

In this exercise you will explore that data frame you created in the previous exercise lineup_k2_complete.

Reminder: The lineup_k2_complete data frame contains the x & y positions of 12 players at the start of a 6v6 soccer game to which you have added clustering assignments based on the following parameters:

Distance: Euclidean
Number of Clusters (k): 2
Linkage Method: Complete
> # Count the cluster assignments
> count(lineup_k2_complete, factor(cluster))
# A tibble: 2 x 2
  `factor(cluster)`     n
  <fct>             <int>
1 1                     6
2 2                     6
> 
> # Plot the positions of the players and color them using their cluster
> ggplot(lineup_k2_complete, aes(x = x, y = y, color = factor(cluster))) +
    geom_point()
> Comparing average, single & complete linkage
You are now ready to analyze the clustering results of the lineup dataset using the dendrogram plot. This will give you a new perspective on the effect the decision of the linkage method has on your resulting cluster analysis.
> # Prepare the Distance Matrix
> dist_players <- dist(lineup)
> 
> # Generate hclust for complete, single & average linkage methods
> hc_complete <- hclust(dist_players,method="complete")
> hc_single <-hclust(dist_players,method="single")
> hc_average <- hclust(dist_players,method="average")
> 
> # Plot & Label the 3 Dendrograms Side-by-Side
> # Hint: To see these Side-by-Side run the 4 lines together as one command
> par(mfrow = c(1,3))
> plot(hc_complete, main = 'Complete Linkage')
> plot(hc_single, main = 'Single Linkage')
> plot(hc_average, main = 'Average Linkage')
> Height of the tree
An advantage of working with a clustering method like hierarchical clustering is that you can describe the relationships between your observations based on both the distance metric and the linkage metric selected (the combination of which defines the height of the tree).


Based on the code below what can you concretely say about the height of a branch in the resulting dendrogram?

dist_players <- dist(lineup, method = 'euclidean')
hc_players <- hclust(dist_players, method = 'single')
plot(hc_players)



All of the observations linked by this branch must have:

Answer the question
50 XP
Possible Answers
a maximum Euclidean distance amongst each other less than or equal to the height of the branch.
press
1
a minimum Jaccard distance amongst each other less than or equal to the height of the branch.
press
2
a minimum Euclidean distance amongst each other less than or equal to the height of the branch.
press
3
Clusters based on height
In previous exercises you have grouped your observations into clusters using a pre-defined number of clusters (k). In this exercise you will leverage the visual representation of the dendrogram in order to group your observations into clusters using a maximum height (h), below which clusters form.

You will work the color_branches() function from the dendextend library in order to visually inspect the clusters that form at any height along the dendrogram.

The hc_players has been carried over from your previous work with the soccer line-up data.
> library(dendextend)
> dist_players <- dist(lineup, method = 'euclidean')
> hc_players <- hclust(dist_players, method = "complete")
> 
> # Create a dendrogram object from the hclust variable
> dend_players <- as.dendrogram(hc_players)
> 
> # Plot the dendrogram
> plot(dend_players)
> 
> # Color branches by cluster formed from the cut at a height of 20 & plot
> dend_20 <- color_branches(dend_players, h =20)
> # Plot the dendrogram with clusters colored below height 20
> plot(dend_20)
> 
> # Color branches by cluster formed from the cut at a height of 40 & plot
> dend_40 <- color_branches(dend_players, h =40)
> 
> # Plot the dendrogram with clusters colored below height 40
> 
> plot(dend_40 )
> > dist_players <- dist(lineup, method = 'euclidean')
> hc_players <- hclust(dist_players, method = "complete")
> 
> # Calculate the assignment vector with a h of 20
> clusters_h20 <- cutree(hc_players ,h=20)
> 
> # Create a new data frame storing these results
> lineup_h20_complete <- mutate(lineup, cluster = clusters_h20)
> 
> # Calculate the assignment vector with a h of 40
> clusters_h40 <- cutree(hc_players ,h=40)
> 
> # Create a new data frame storing these results
> lineup_h40_complete <- mutate(lineup, cluster = clusters_h40)
> 
> # Plot the positions of the players and color them using their cluster for height = 20
> ggplot(lineup_h20_complete, aes(x = x, y = y, color = factor(cluster))) +
    geom_point()
> 
> # Plot the positions of the players and color them using their cluster for height = 40
> ggplot(lineup_h40_complete, aes(x = x, y = y, color = factor(cluster))) +
    geom_point()
> #
What do we know about our clusters?
Based on the code below, what can you concretely say about the relationships of the members within each cluster?

dist_players <- dist(lineup, method = 'euclidean')
hc_players <- hclust(dist_players, method = 'complete')
clusters <- cutree(hc_players, h = 40)



Every member belonging to a cluster must have:

Answer the question
50 XP
Possible Answers
a maximum Euclidean distance to all other members of its cluster that is less than 40.
press
1
a maximum Euclidean distance to all other members of its cluster that is greater than or equal to 40.
press
2
a average Euclidean distance to all other members of its cluster that is less than 40.
press
3
#Segment wholesale customers
You're now ready to use hierarchical clustering to perform market segmentation (i.e. use consumer characteristics to group them into subgroups).

In this exercise you are provided with the amount spent by 45 different clients of a wholesale distributor for the food categories of Milk, Grocery & Frozen. This is stored in the data frame customers_spend. Assign these clients into meaningful clusters.
Note: For this exercise you can assume that because the data is all of the same type (amount spent) and you will not need to scale it.
> # Calculate Euclidean distance between customers
> dist_customers <- dist(customers_spend)
> 
> # Generate a complete linkage analysis
> hc_customers <- hclust(dist_customers,method="complete")
> 
> # Plot the dendrogram
> plot(hc_customers)
> 
> # Create a cluster assignment vector at h = 15000
> clust_customers <-cutree(hc_customers,h=15000)
> 
> # Generate the segmented customers data frame
> segment_customers <- mutate(customers_spend, cluster = clust_customers)
> > dist_customers <- dist(customers_spend)
> hc_customers <- hclust(dist_customers)
> clust_customers <- cutree(hc_customers, h = 15000)
> segment_customers <- mutate(customers_spend, cluster = clust_customers)
> 
> # Count the number of customers that fall into each cluster
> count(segment_customers, cluster)
# A tibble: 4 x 2
  cluster     n
    <int> <int>
1       1     5
2       2    29
3       3     5
4       4     6
> 
> # Color the dendrogram based on the height cutoff
> dend_customers <- as.dendrogram(hc_customers)
> dend_colored <- color_branches(dend_customers, h=15000)
> 
> # Plot the colored dendrogram
> plot(dend_colored)
> 
> # Calculate the mean for each category
> segment_customers %>% 
    group_by(cluster) %>% 
    summarise_all(funs(mean(.)))
# A tibble: 4 x 4
  cluster   Milk Grocery Frozen
    <int>  <dbl>   <dbl>  <dbl>
1       1 16950   12891.   991.
2       2  2513.   5229.  1796.
3       3 10452.  22551.  1355.
4       4  1250.   3917. 10889.
> #Interpreting the wholesale customer clusters
What observations can we make about our segments based on their average spending in each category?

cluster	Milk	Grocery	Frozen	cluster size
1	16950	12891	991	5
2	2512	5228	1795	29
3	10452	22550	1354	5
4	1249	3916	10888	6
Answer the question
50 XP
Possible Answers
Customers in cluster 1 spent more money on Milk than any other cluster.
press
1
Customers in cluster 3 spent more money on Grocery than any other cluster.
press
2
Customers in cluster 4 spent more money on Frozen goods than any other cluster.
press
3
The majority of customers fell into cluster 2 and did not show any excessive spending in any category.
press
4
All of the above.
press
#chapter3-K-means
K-means on a soccer field
In the previous chapter you used the lineup dataset to learn about hierarchical clustering, in this chapter you will use the same data to learn about k-means clustering. As a reminder, the lineup data frame contains the positions of 12 players at the start of a 6v6 soccer match.

Just like before, you know that this match has two teams on the field so you can perform a k-means analysis using k = 2 in order to determine which player belongs to which team.

Note that in the kmeans() function k is specified using the centers parameter.
> # Build a kmeans model
> model_km2 <- kmeans(lineup, centers = 2)
> 
> # Extract the cluster assignment vector from the kmeans model
> clust_km2 <- model_km2$cluster
> 
> # Create a new data frame appending the cluster assignment
> lineup_km2 <- mutate(lineup, cluster = clust_km2)
> 
> # Plot the positions of the players and color them using their cluster
> ggplot(lineup_km2, aes(x = x, y = y, color = factor(cluster))) +
    geom_point()
> 
K-means on a soccer field (part 2)
In the previous exercise you successfully used the k-means algorithm to cluster the two teams from the lineup data frame. This time, let's explore what happens when you use a k of 3.

You will see that the algorithm will still run, but does it actually make sense in this context...
> # Build a kmeans model
> model_km3 <- kmeans(lineup,centers=3)
> 
> # Extract the cluster assignment vector from the kmeans model
> clust_km3 <- model_km3$cluster
> 
> # Create a new data frame appending the cluster assignment
> lineup_km3 <- mutate(lineup,cluster=clust_km3)
> 
> # Plot the positions of the players and color them using their cluster
> ggplot(lineup_km3, aes(x = x, y = y, color = factor(cluster))) +
    geom_point()
> Many K's many models
While the lineup dataset clearly has a known value of k, often times the optimal number of clusters isn't known and must be estimated.

In this exercise you will leverage map_dbl() from the purrr library to run k-means using values of k ranging from 1 to 10 and extract the total within-cluster sum of squares metric from each one. This will be the first step towards visualizing the elbow plot.
#> library(purrr)
> 
> # Use map_dbl to run many models with varying value of k (centers)
> tot_withinss <- map_dbl(1:10,  function(k){
    model <- kmeans(x = lineup, centers = k)
    model$tot.withinss
  })
> 
> # Generate a data frame containing both k and tot_withinss
> elbow_df <- data.frame(
    k = 1:10 ,
    tot_withinss = tot_withinss
  )
  #Elbow (Scree) plot
In the previous exercises you have calculated the total within-cluster sum of squares for values of k ranging from 1 to 10. You can visualize this relationship using a line plot to create what is known as an elbow plot (or scree plot).

When looking at an elbow plot you want to see a sharp decline from one k to another followed by a more gradual decrease in slope. The last value of k before the slope of the plot levels off suggests a "good" value of k.
> # Use map_dbl to run many models with varying value of k (centers)
> tot_withinss <- map_dbl(1:10,  function(k){
    model <- kmeans(x = lineup, centers = k)
    model$tot.withinss
  })
> 
> # Generate a data frame containing both k and tot_withinss
> elbow_df <- data.frame(
    k = 1:10,
    tot_withinss = tot_withinss
  )
> 
> # Plot the elbow plot
> ggplot(elbow_df, aes(x = k, y = tot_withinss)) +
    geom_line() +
    scale_x_continuous(breaks = 1:10)
> #Silhouette analysis
Silhouette analysis allows you to calculate how similar each observations is with the cluster it is assigned relative to other clusters. This metric (silhouette width) ranges from -1 to 1 for each observation in your data and can be interpreted as follows:

Values close to 1 suggest that the observation is well matched to the assigned cluster
Values close to 0 suggest that the observation is borderline matched between two clusters
Values close to -1 suggest that the observations may be assigned to the wrong cluster
In this exercise you will leverage the pam() and the silhouette() functions from the cluster library to perform silhouette analysis to compare the results of models with a k of 2 and a k of 3. You'll continue working with the lineup dataset.

Pay close attention to the silhouette plot, does each observation clearly belong to its assigned cluster for k = 3?
> library(cluster)
> 
> # Generate a k-means model using the pam() function with a k = 2
> pam_k2 <- pam(lineup, k = 2)
> 
> # Plot the silhouette visual for the pam_k2 model
> plot(silhouette(pam_k2))
> 
> # Generate a k-means model using the pam() function with a k = 3
> pam_k3 <- pam(lineup,k=3)
> 
> # Plot the silhouette visual for the pam_k3 model
> plot(silhouette(pam_k3))
> Revisiting wholesale data: "Best" k
At the end of Chapter 2 you explored wholesale distributor data customers_spend using hierarchical clustering. This time you will analyze this data using the k-means clustering tools covered in this chapter.

The first step will be to determine the "best" value of k using average silhouette width.

A refresher about the data: it contains records of the amount spent by 45 different clients of a wholesale distributor for the food categories of Milk, Grocery & Frozen. This is stored in the data frame customers_spend. For this exercise you can assume that because the data is all of the same type (amount spent) and you will not need to scale it.
> # Use map_dbl to run many models with varying value of k
> sil_width <- map_dbl(2:10,  function(k){
    model <- pam(x = customers_spend, k = k)
    model$silinfo$avg.width
  })
> 
> # Generate a data frame containing both k and sil_width
> sil_df <- data.frame(
    k = 2:10,
    sil_width = sil_width
  )
> 
> # Plot the relationship between k and sil_width
> ggplot(sil_df, aes(x = k, y = sil_width)) +
    geom_line() +
    scale_x_continuous(breaks = 2:10)
> Revisiting wholesale data: Exploration
From the previous analysis you have found that k = 2 has the highest average silhouette width. In this exercise you will continue to analyze the wholesale customer data by building and exploring a kmeans model with 2 clusters.
> set.seed(42)
> 
> # Build a k-means model for the customers_spend with a k of 2
> model_customers <- kmeans(customers_spend,centers = 2)
> 
> # Extract the vector of cluster assignments from the model
> clust_customers <- model_customers$cluster
> 
> # Build the segment_customers data frame
> segment_customers <- mutate(customers_spend, cluster = clust_customers)
> 
> # Calculate the size of each cluster
> count(segment_customers ,cluster)
# A tibble: 2 x 2
  cluster     n
    <int> <int>
1       1    35
2       2    10
> 
> # Calculate the mean for each category
> segment_customers %>% 
    group_by(cluster) %>% 
    summarise_all(funs(mean(.)))
# A tibble: 2 x 4
  cluster   Milk Grocery Frozen
    <int>  <dbl>   <dbl>  <dbl>
1       1  2296.    5004  3354.
2       2 13701.   17721  1173
> 
#chapter4-Case Study-National Occupational mean wage
Initial exploration of the data
You are presented with data from the Occupational Employment Statistics (OES) program which produces employment and wage estimates annually. This data contains the yearly average income from 2001 to 2016 for 22 occupation groups. You would like to use this data to identify clusters of occupations that maintained similar income trends.

The data is stored in your environment as the data.matrix oes.

Before you begin to cluster this data you should determine whether any pre-processing steps (such as scaling and imputation) are necessary.

Leverage the functions head() and summary() to explore the oes data in order to determine which of the pre-processing steps below are necessary:

Instructions
50 XP
Possible Answers
NA values exist in the data, hence the values must be imputed or the observations with NAs excluded.
The variables within this data are not comparable to one another and should be scaled.
Categorical variables exist within this data and should be appropriately dummified.
All three pre-processing steps above are necessary for this data.
None of these pre-processing steps are necessary for this data.
#Hierarchical clustering: Occupation trees
In the previous exercise you have learned that the oes data is ready for hierarchical clustering without any preprocessing steps necessary. In this exercise you will take the necessary steps to build a dendrogram of occupations based on their yearly average salaries and propose clusters using a height of 100,000.
> # Calculate Euclidean distance between the occupations
> dist_oes <- dist(oes, method = "euclidean")
> 
> # Generate an average linkage analysis
> hc_oes <- hclust(dist_oes, method = "average")
> 
> # Create a dendrogram object from the hclust variable
> dend_oes <- as.dendrogram(hc_oes)
> 
> # Plot the dendrogram
> plot(dend_oes)
> 
> # Color branches by cluster formed from the cut at a height of 100000
> dend_colored <- color_branches(dend_oes, h = 100000)
> 
> # Plot the colored dendrogram
> plot(dend_colored)
> Exercise
Exercise
Hierarchical clustering: Preparing for exploration
You have now created a potential clustering for the oes data, before you can explore these clusters with ggplot2 you will need to process the oes data matrix into a tidy data frame with each occupation assigned its cluster
> dist_oes <- dist(oes, method = 'euclidean')
> hc_oes <- hclust(dist_oes, method = 'average')
> 
> library(tibble)
> library(tidyr)
> 
> # Use rownames_to_column to move the rownames into a column of the data frame
> df_oes <- rownames_to_column(as.data.frame(oes), var = 'occupation')
> 
> # Create a cluster assignment vector at h = 100,000
> cut_oes <- cutree(hc_oes, h = 100000)
> 
> # Generate the segmented the oes data frame
> clust_oes <- mutate(df_oes, cluster = cut_oes)
> 
> # Create a tidy data frame by gathering the year and values into two columns
> gathered_oes <- gather(data = clust_oes, 
                         key = year, 
                         value = mean_salary, 
                         -occupation, -cluster)
> Hierarchical clustering: Plotting occupational clusters
You have succesfully created all the parts necessary to explore the results of this hierarchical clustering work. In this exercise you will leverage the named assignment vector cut_oes and the tidy data frame gathered_oes to analyze the resulting clusters.
> # View the clustering assignments by sorting the cluster assignment vector
> sort(cut_oes)
                Management                      Legal 
                         1                          1 
       Business Operations           Computer Science 
                         2                          2 
  Architecture/Engineering  Life/Physical/Social Sci. 
                         2                          2 
  Healthcare Practitioners         Community Services 
                         2                          3 
Education/Training/Library  Arts/Design/Entertainment 
                         3                          3 
        Healthcare Support         Protective Service 
                         3                          3 
          Food Preparation  Grounds Cleaning & Maint. 
                         3                          3 
             Personal Care                      Sales 
                         3                          3 
     Office Administrative   Farming/Fishing/Forestry 
                         3                          3 
              Construction Installation/Repair/Maint. 
                         3                          3 
                Production      Transportation/Moving 
                         3                          3
> 
> # Plot the relationship between mean_salary and year and color the lines by the assigned cluster
> ggplot(gathered_oes, aes(x = year, y = mean_salary, color = factor(cluster))) + 
      geom_line(aes(group = occupation))
> K-means: Elbow analysis
In the previous exercises you used the dendrogram to propose a clustering that generated 3 trees. In this exercise you will leverage the k-means elbow plot to propose the "best" number of clusters.
> # Use map_dbl to run many models with varying value of k (centers)
> tot_withinss <- map_dbl(1:10,  function(k){
    model <- kmeans(x = oes, centers = k)
    model$tot.withinss
  })
> 
> # Generate a data frame containing both k and tot_withinss
> elbow_df <- data.frame(
    k = 1:10,
    tot_withinss =tot_withinss
  )
> 
> # Plot the elbow plot
> ggplot(elbow_df, aes(x = k, y = tot_withinss)) +
    geom_line() +
    scale_x_continuous(breaks = 1:10)
> K-means: Average Silhouette Widths
So hierarchical clustering resulting in 3 clusters and the elbow method suggests 2. In this exercise use average silhouette widths to explore what the "best" value of k should be.
> # Use map_dbl to run many models with varying value of k
> sil_width <- map_dbl(2:10,  function(k){
    model <- pam(oes, k = k)
    model$silinfo$avg.width
  })
> 
> # Generate a data frame containing both k and sil_width
> sil_df <- data.frame(
    k = 2:10,
    sil_width = sil_width
  )
> 
> # Plot the relationship between k and sil_width
> ggplot(sil_df, aes(x = k, y = sil_width)) +
    geom_line() +
    scale_x_continuous(breaks = 2:10)
> 
